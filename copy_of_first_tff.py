# -*- coding: utf-8 -*-
"""Copy of first_tff.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N87ps-OowlnrQTGZkTvRRn9weU-y7Kf8
"""

!pip install  --quiet  tensorflow-federated-nightly
!pip install  --quiet  nest-asyncio

import tensorflow_federated as tff
import tensorflow as tf
import numpy as np
import collections
import nest_asyncio
import platform

nest_asyncio.apply()
(x, y), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x = np.array(x, dtype=np.float32) / 255
x_test = np.array(x_test, dtype=np.float32) / 255
x=np.expand_dims(x,-1)
x_test=np.expand_dims(x_test,-1)
train=collections.OrderedDict(
        pixels =x,
        label=y)
print(tf.__version__)
print(tff.__version__)
print(platform.python_version())

NUM_CLIENTS = 1
NUM_EPOCHS = 5
BATCH_SIZE = 20
SHUFFLE_BUFFER = 100
PREFETCH_BUFFER = 10
def preprocess(dataset):
  def batch_format_fn(element):
    """Flatten a batch `pixels` and return the features as an `OrderedDict`."""
    return collections.OrderedDict(
        x=element['pixels'],
        y=tf.reshape(element['label'], [-1, 1]))
  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(
      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)
dataset = tf.data.Dataset.from_tensor_slices(train)
dataset

client_ids=[ "f"+str(i) for i in range(10)]
def create_tf_dataset_for_client_fn(client_id):
  # a function which takes a client_id and returns a
  # tf.data.Dataset for that client 
  # Todo we do not know how to split dataset properly.
  return dataset

client_data = tff.simulation.datasets.inaturalist.ClientData.from_clients_and_tf_fn( client_ids=client_ids,
        serializable_dataset_fn=create_tf_dataset_for_client_fn)
example_dataset = client_data.create_tf_dataset_for_client(
    client_data.client_ids[0])
preprocessed_example_dataset = preprocess(example_dataset)
def make_federated_data(client_data, client_ids):
  return [
      preprocess(client_data.create_tf_dataset_for_client(x))
      for x in client_ids
  ]

sample_clients = client_data.client_ids[0:NUM_CLIENTS]
federated_train_data = make_federated_data(client_data, sample_clients)
print('Number of client datasets: {l}'.format(l=len(federated_train_data)))
print('First dataset: {d}'.format(d=federated_train_data[0]))

def create_keras_model():
  model = tf.keras.models.Sequential()
  model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))
  model.add(tf.keras.layers.MaxPooling2D((2, 2)))
  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(tf.keras.layers.MaxPooling2D((2, 2)))
  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.Dense(64, activation='relu'))
  model.add(tf.keras.layers.Dense(10))
  return model

def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
    input_spec=preprocessed_example_dataset.element_spec ,
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
  
iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),
    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))
str(iterative_process.initialize.type_signature)

state = iterative_process.initialize()

state, metrics = iterative_process.next(state, federated_train_data)
print('round  1, metrics={}'.format(metrics))

NUM_ROUNDS = 11
for round_num in range(2, NUM_ROUNDS):
  state, metrics = iterative_process.next(state, federated_train_data)
  print('round {:2d}, metrics={}'.format(round_num, metrics))